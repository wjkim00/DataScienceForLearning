{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Dense Layers"
      ],
      "metadata": {
        "id": "Cx2v0QOqlF6v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shapes of Dense Layers"
      ],
      "metadata": {
        "id": "4Mf_VMSwuBec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "N, n_feature = 8, 10\n",
        "X = tf.random.normal(shape=(N, n_feature))\n",
        "\n",
        "n_neuron = 3\n",
        "dense = Dense(units=n_neuron, activation='sigmoid')\n",
        "Y = dense(X)\n",
        "\n",
        "W, B = dense.get_weights()\n",
        "\n",
        "print('======================')\n",
        "print(\"X: \", X.shape)\n",
        "print(\"W: \", W.shape)\n",
        "print(\"B: \", B.shape)\n",
        "print(\"Y: \", Y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18LmlsNnuDjf",
        "outputId": "25619766-d057-4bd7-ad2e-f7c2c5c7ee30"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================\n",
            "X:  (8, 10)\n",
            "W:  (10, 3)\n",
            "B:  (3,)\n",
            "Y:  (8, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculation"
      ],
      "metadata": {
        "id": "okfygT9vwD5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.math import exp\n",
        "from tensorflow.linalg import matmul\n",
        "\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "N, n_feature = 4, 10\n",
        "X = tf.random.normal(shape=(N, n_feature))\n",
        "\n",
        "n_neuron = 3\n",
        "dense = Dense(units=n_neuron, activation='sigmoid')\n",
        "Y_tf = dense(X)\n",
        "\n",
        "W, B = dense.get_weights()\n",
        "print(\"Y(Tensorflow): \\n\", Y_tf.numpy())\n",
        "\n",
        "Z = matmul(X, W) + B\n",
        "Y_man_matmul = 1 / (1 + exp(-Z))\n",
        "print(\"Y(with matrix multiplication: \\n\", Y_man_matmul.numpy())\n",
        "\n",
        "# dot product\n",
        "Y_man_vec = np.zeros(shape=(N, n_neuron))\n",
        "for x_idx in range(N):\n",
        "  x = X[x_idx]\n",
        "\n",
        "  for nu_idx in range(n_neuron):\n",
        "    w, b = W[:, nu_idx], B[nu_idx]\n",
        "\n",
        "    z = tf.reduce_sum(x * w) + b\n",
        "    a = 1/(1 + np.exp(-z))\n",
        "    Y_man_vec[x_idx, nu_idx] = a\n",
        "\n",
        "print(\"Y(with dot products): \\n\", Y_man_vec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0K5SheI7u1Sm",
        "outputId": "9419c52a-94be-4046-8e3b-52cfd2e5123b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Y(Tensorflow): \n",
            " [[0.5784518  0.72433925 0.377539  ]\n",
            " [0.16027576 0.83551407 0.90179217]\n",
            " [0.24797311 0.35391504 0.36988705]\n",
            " [0.4280033  0.57778275 0.83454084]]\n",
            "Y(with matrix multiplication: \n",
            " [[0.5784519  0.7243393  0.37753898]\n",
            " [0.16027579 0.83551407 0.90179217]\n",
            " [0.24797308 0.35391504 0.36988705]\n",
            " [0.4280033  0.57778275 0.8345409 ]]\n",
            "Y(with dot products): \n",
            " [[0.57845186 0.72433925 0.37753898]\n",
            " [0.16027579 0.83551409 0.90179219]\n",
            " [0.24797305 0.35391501 0.36988704]\n",
            " [0.42800331 0.57778278 0.83454089]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cascaded Dense Layers"
      ],
      "metadata": {
        "id": "v3RAx1uKwOcs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shapes of Cascaded Dense Layers"
      ],
      "metadata": {
        "id": "irBqaDAkwRla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "N, n_feature = 4, 10\n",
        "X = tf.random.normal(shape=(N, n_feature))\n",
        "\n",
        "n_neurons = (3, 5)\n",
        "dense1 = Dense(units=n_neurons[0], activation='sigmoid')\n",
        "dense2 = Dense(units=n_neurons[1], activation='sigmoid')\n",
        "\n",
        "A1 = dense1(X)\n",
        "Y = dense2(A1)\n",
        "\n",
        "W1, B1 = dense1.get_weights()\n",
        "W2, B2 = dense2.get_weights()\n",
        "\n",
        "\n",
        "print(\"X: {}\\n\".format(X.shape))\n",
        "\n",
        "print(\"W1: \", W1.shape)\n",
        "print(\"B1: \", B1.shape)\n",
        "print(\"A1: {}\\n\".format(A1.shape))\n",
        "\n",
        "print(\"W2: \", W2.shape)\n",
        "print(\"B2: \", B2.shape)\n",
        "print(\"Y: {}\\n\".format(Y.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrlRAYfLwUOA",
        "outputId": "118afcef-0c91-426e-e85d-5a3ae35022e6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X: (4, 10)\n",
            "\n",
            "W1:  (10, 3)\n",
            "B1:  (3,)\n",
            "A1: (4, 3)\n",
            "\n",
            "W2:  (3, 5)\n",
            "B2:  (5,)\n",
            "Y: (4, 5)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dense Layers with Python List"
      ],
      "metadata": {
        "id": "eoEyzTEZxdbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "N, n_feature = 4, 10\n",
        "X = tf.random.normal(shape=(N, n_feature))\n",
        "\n",
        "n_neurons = [10, 20, 30, 40]\n",
        "\n",
        "dense_layers = list()\n",
        "for n_neuron in n_neurons:\n",
        "  dense = Dense(units=n_neuron, activation='relu')\n",
        "  dense_layers.append(dense)\n",
        "\n",
        "print(\"Input: \", X.shape)\n",
        "for dense_idx, dense in enumerate(dense_layers):\n",
        "  X = dense(X)\n",
        "  print(\"After dense layer \", dense_idx + 1)\n",
        "  print(X.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGeKXkOkxfYl",
        "outputId": "d4f62a31-d4ac-4cec-960b-0aa3371e0ccf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:  (4, 10)\n",
            "After dense layer  1\n",
            "(4, 10)\n",
            "After dense layer  2\n",
            "(4, 20)\n",
            "After dense layer  3\n",
            "(4, 30)\n",
            "After dense layer  4\n",
            "(4, 40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output "
      ],
      "metadata": {
        "id": "MiHma4J4yY1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.math import exp\n",
        "from tensorflow.linalg import matmul\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "N, n_feature = 4, 10\n",
        "X = tf.random.normal(shape=(N, n_feature))\n",
        "X_cp = tf.identity(X)\n",
        "\n",
        "n_neurons = [10, 20, 30, 40]\n",
        "\n",
        "dense_layers = list()\n",
        "for n_neuron in n_neurons:\n",
        "  dense = Dense(units=n_neuron, activation='sigmoid')\n",
        "  dense_layers.append(dense)\n",
        "\n",
        "print(\"Input: \", X.shape)\n",
        "W, B = list(), list()\n",
        "for dense_idx, dense in enumerate(dense_layers):\n",
        "  X = dense(X)\n",
        "  w, b = dense.get_weights()\n",
        "\n",
        "  W.append(w)\n",
        "  B.append(b)\n",
        "print(\"Y(Tensorflow): \\n\", X.numpy())\n",
        "\n",
        "for layer_idx in range(len(n_neurons)):\n",
        "  w, b = W[layer_idx], B[layer_idx]\n",
        "\n",
        "  X_cp = matmul(X_cp, w) + b\n",
        "  X_cp = 1/(1 + exp(-X_cp))\n",
        "print(\"Y(Manual): \\n\", X_cp.numpy())\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2hgALHvyaDf",
        "outputId": "9a3051f8-2817-403f-9939-6f8cde212762"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:  (4, 10)\n",
            "Y(Tensorflow): \n",
            " [[0.44352597 0.45499492 0.3942266  0.52089745 0.57572144 0.40692788\n",
            "  0.53828263 0.4884545  0.55251884 0.5413941  0.21682218 0.249571\n",
            "  0.37931544 0.52255005 0.5798491  0.63713855 0.3363046  0.5585452\n",
            "  0.5524795  0.6677488  0.38821274 0.3678243  0.49819285 0.38521326\n",
            "  0.46751624 0.32561356 0.38134378 0.4140711  0.43932244 0.34939677\n",
            "  0.6103392  0.47480214 0.46267134 0.40070432 0.47640336 0.33497575\n",
            "  0.64610195 0.62067723 0.65816015 0.5627639 ]\n",
            " [0.4443699  0.45646194 0.39684454 0.52189785 0.5763557  0.40712523\n",
            "  0.53853774 0.4902177  0.55416435 0.54252464 0.21633413 0.2509538\n",
            "  0.3815214  0.52392197 0.58067024 0.6363623  0.33647627 0.55990046\n",
            "  0.5537112  0.6663436  0.38793847 0.36944795 0.49612167 0.38516927\n",
            "  0.46899274 0.32575867 0.38594913 0.41613698 0.43681923 0.351642\n",
            "  0.6113871  0.47502846 0.46554297 0.40225837 0.4746237  0.33399135\n",
            "  0.6474723  0.6201634  0.66015923 0.56267715]\n",
            " [0.44308212 0.4552146  0.39887387 0.52262145 0.5809182  0.41220498\n",
            "  0.53673977 0.48509306 0.5561126  0.53860205 0.218804   0.25022122\n",
            "  0.38142985 0.52473336 0.58493066 0.63419294 0.33452332 0.5595879\n",
            "  0.55248547 0.6660435  0.38660526 0.36991006 0.49830562 0.38684627\n",
            "  0.47538573 0.3302848  0.3907097  0.41639513 0.43418628 0.35218778\n",
            "  0.61073846 0.4744163  0.4629895  0.40607154 0.4708076  0.33255523\n",
            "  0.6452147  0.6180031  0.65733105 0.56719005]\n",
            " [0.43887472 0.45746672 0.39448172 0.5252177  0.5726284  0.4118014\n",
            "  0.5372276  0.48615468 0.5508564  0.5369954  0.21821329 0.25131303\n",
            "  0.3773727  0.52643096 0.5827819  0.6349057  0.33203578 0.56025475\n",
            "  0.5511433  0.6670717  0.3897203  0.37154847 0.5020949  0.387048\n",
            "  0.4730673  0.32669598 0.3813866  0.4126433  0.43700844 0.35320136\n",
            "  0.61043465 0.47432286 0.46270698 0.40301222 0.47173917 0.3353752\n",
            "  0.6443379  0.6181859  0.655453   0.5641645 ]]\n",
            "Y(Manual): \n",
            " [[0.4435259  0.4549949  0.3942266  0.5208974  0.57572144 0.40692785\n",
            "  0.53828263 0.4884545  0.5525189  0.5413942  0.21682215 0.24957098\n",
            "  0.37931544 0.52255005 0.5798491  0.63713855 0.33630458 0.5585452\n",
            "  0.5524795  0.6677488  0.38821274 0.3678243  0.49819285 0.38521326\n",
            "  0.4675162  0.32561353 0.38134378 0.41407108 0.43932244 0.34939677\n",
            "  0.6103392  0.47480217 0.4626714  0.4007043  0.47640333 0.33497572\n",
            "  0.64610195 0.6206772  0.65816015 0.5627639 ]\n",
            " [0.4443699  0.45646194 0.39684454 0.52189785 0.5763557  0.40712523\n",
            "  0.5385378  0.4902177  0.55416435 0.54252464 0.21633413 0.25095376\n",
            "  0.3815214  0.52392197 0.58067024 0.6363623  0.33647627 0.55990046\n",
            "  0.5537112  0.6663437  0.38793844 0.36944795 0.49612167 0.38516924\n",
            "  0.4689927  0.32575864 0.38594908 0.416137   0.43681923 0.35164198\n",
            "  0.6113871  0.47502849 0.465543   0.4022584  0.47462374 0.33399135\n",
            "  0.6474724  0.6201634  0.66015923 0.56267715]\n",
            " [0.44308212 0.4552146  0.39887387 0.52262145 0.58091813 0.41220495\n",
            "  0.53673977 0.48509303 0.5561125  0.53860205 0.21880399 0.25022122\n",
            "  0.38142985 0.52473336 0.5849307  0.63419294 0.3345233  0.5595879\n",
            "  0.55248547 0.6660435  0.38660526 0.36991003 0.4983056  0.3868463\n",
            "  0.47538567 0.33028477 0.39070967 0.41639513 0.43418622 0.35218775\n",
            "  0.6107385  0.4744163  0.46298948 0.40607154 0.4708076  0.33255523\n",
            "  0.6452147  0.6180031  0.65733105 0.5671901 ]\n",
            " [0.43887475 0.4574667  0.39448172 0.5252177  0.5726284  0.41180137\n",
            "  0.5372276  0.48615465 0.5508564  0.5369955  0.21821326 0.25131303\n",
            "  0.37737268 0.526431   0.5827819  0.6349057  0.33203575 0.56025475\n",
            "  0.5511433  0.6670717  0.38972026 0.37154847 0.502095   0.38704795\n",
            "  0.47306734 0.32669598 0.38138664 0.4126433  0.4370084  0.35320133\n",
            "  0.61043465 0.47432283 0.46270695 0.40301225 0.4717392  0.33537513\n",
            "  0.6443379  0.6181859  0.65545297 0.5641646 ]]\n"
          ]
        }
      ]
    }
  ]
}